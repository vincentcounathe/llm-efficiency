# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hj-D06tVkUL3Pmw-xOX1Q9LHnFq9WAD
"""

# ==============================================
#   LLM Efficiency
#   Full FT vs LoRA vs Quant
# ==============================================

!pip install -q transformers datasets peft bitsandbytes accelerate evaluate matplotlib

import torch
import gc
import matplotlib.pyplot as plt
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import evaluate

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ----------------------------------------------------
# 1. dataset
# ----------------------------------------------------
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
train_data = dataset["train"].select(range(300))  # just to run fast
test_data = dataset["test"].select(range(100))

# ----------------------------------------------------
# 2. tokenizer setup
# ----------------------------------------------------
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize_fn(examples):
    # quick tokenize
    result = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=64,   # tiny for speed
    )
    result["labels"] = result["input_ids"].copy()
    return result

tokenized_train = train_data.map(tokenize_fn, batched=True, remove_columns=["text"])
tokenized_test = test_data.map(tokenize_fn, batched=True, remove_columns=["text"])

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# ----------------------------------------------------
# 3. full fine-tuning baseline ( one epoch)
# ----------------------------------------------------
print("\n=== Full FT ===")
full_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
full_trainable_params = sum(p.numel() for p in full_model.parameters() if p.requires_grad)
full_total_params = sum(p.numel() for p in full_model.parameters())
print(f"Trainable params: {full_trainable_params:,}/{full_total_params:,} "
      f"({100*full_trainable_params/full_total_params:.2f}%)")

full_args = TrainingArguments(
    output_dir="./full_ft",
    overwrite_output_dir=True,
    num_train_epochs=1,  # just one epoch
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    eval_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="no",
    report_to="none",
    fp16=torch.cuda.is_available(),
)

full_trainer = Trainer(
    model=full_model,
    args=full_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    data_collator=data_collator,
)

full_train_result = full_trainer.train()
full_eval = full_trainer.evaluate()

# ----------------------------------------------------
# 4. LoRA fine-tuning
# ----------------------------------------------------
print("\n=== LoRA FT ===")
lora_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

lora_config = LoraConfig(
    r=4,   # to be fast
    lora_alpha=16,
    target_modules=["attn.c_attn", "attn.c_proj"],  # distilgpt2
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

lora_model = prepare_model_for_kbit_training(lora_model)
lora_model = get_peft_model(lora_model, lora_config)
lora_model.print_trainable_parameters()

lora_args = TrainingArguments(
    output_dir="./lora_ft",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    eval_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="no",
    report_to="none",
    fp16=torch.cuda.is_available(),
)

lora_trainer = Trainer(
    model=lora_model,
    args=lora_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    data_collator=data_collator,
)

lora_train_result = lora_trainer.train()
lora_eval = lora_trainer.evaluate()

# ----------------------------------------------------
# 5. Quantization
# ----------------------------------------------------
print("\n=== Quantization Playground ===")
from transformers import BitsAndBytesConfig

quant_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto" if torch.cuda.is_available() else None,
    quantization_config=BitsAndBytesConfig(load_in_8bit=True)
)

before_size = sum(p.numel() for p in full_model.parameters()) * 4 / (1024**2)
after_size = sum(p.numel() for p in quant_model.parameters()) * 1 / (1024**2)
print(f"Model size before quant: {before_size:.2f} MB")
print(f"Model size after  quant: {after_size:.2f} MB")

# ppl calc
sample_texts = [t for t in test_data["text"][:10] if len(tokenizer.encode(t)) > 1]

import math

def compute_ppl(model, tokenizer, texts):
    model.eval()
    ppl_list = []
    for txt in texts:
        inputs = tokenizer(txt, return_tensors="pt", truncation=True, max_length=64).to(device)
        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss
        ppl_list.append(torch.exp(loss).item())
    return sum(ppl_list)/len(ppl_list)

full_ppl = compute_ppl(full_model, tokenizer, sample_texts)
quant_ppl = compute_ppl(quant_model, tokenizer, sample_texts)
print(f"PPL - Full: {full_ppl:.2f}, Quant: {quant_ppl:.2f}")

# ----------------------------------------------------
# 6. skip charts
# ----------------------------------------------------
# plt.figure(figsize=(7,5))
# plt.plot(full_train_result.training_loss, label="Full FT Loss")
# plt.plot(lora_train_result.training_loss, label="LoRA FT Loss")
# plt.xlabel("Steps")
# plt.ylabel("Training Loss")
# plt.title("LoRA vs Full FT")
# plt.legend()
# plt.show()

# cleanup stuff
del full_model, lora_model, quant_model
gc.collect()
torch.cuda.empty_cache() if torch.cuda.is_available() else None

print("\n=== DONE ===")
print(f"Full FT eval loss: {full_eval['eval_loss']:.2f}")
print(f"LoRA eval loss: {lora_eval['eval_loss']:.2f}")